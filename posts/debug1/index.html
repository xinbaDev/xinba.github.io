<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>记一次生产服务器debug(上) &#183; Alex's Blog</title><link rel=stylesheet href=/xinba.github.io/css/style.css><link rel=stylesheet href=/xinba.github.io/css/fonts.css><link rel=icon href=favicon.ico><link rel=icon type=image/png sizes=32x32 href=/xinba.github.io/images/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/xinba.github.io/images/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/xinba.github.io/images/apple-touch-icon.png><link href rel=alternate type=application/rss+xml title="Alex's Blog"></head><body><nav class=nav><div class=nav-container><a href=/xinba.github.io/><h2 class=nav-title>Alex's Blog</h2></a><ul><li><a href=/xinba.github.io/posts><span>Posts</span></a></li></ul></div></nav><main><div class=post><div class=post-info><span>Written by</span>
Alex<br><span>on&nbsp;</span><time datetime="2020-04-14 10:27:22 +1100 +1100">April 14, 2020</time></div><h1 class=post-title>记一次生产服务器debug(上)</h1><div class=post-line></div><p>线索：</p><p><strong>开始修复前：</strong></p><ol><li>17:18分，nginx的error日志里开始出现upstream 110 connection timeout的记录。</li><li>17:22分， netdata开始报警20min steal cpu = 18.2%， 并很快上升31.8%</li><li>17:27分， 群里开始有人报告相应服务不能访问</li><li>17:30分， 远程登录服务器后发现cpu使用率非常的高（主要是node server在占用），基本在80-100使用率，但是服务器的访问量并不高。</li></ol><p><strong>开始修复：</strong></p><ol><li>重启production的node server，并关闭对应的test服务。问题依旧存在。单纯应用出错的可能性基本排除。</li><li>因为之前发生过admin输入错误的tag导致应用不能正常运行的情况，而且根据nginx的记录是17：18才开始突然出现，怀疑db里的数据出了问题。于是import了staging的db的数据，但是问题依旧存在。排除db数据出错的原因。</li><li>因为短时间找不到问题的根源，怀疑问题可能出现在比较底层的地方。于是决定先迁移到新的服务器，使服务先可以访问再说。迁移过去后访问恢复正常。但是即使在很低访问量的情况下<strong>cpu的使用率依旧很高</strong>。</li></ol><p><strong>恢复访问后的debug:</strong></p><ol><li>在旧的服务器上对staging server进行压力测试，1秒10+个request，持续半个小时，使cpu保持在80-100%之间。但是无法重现服务无法访问的情况, steal cpu基本正常。为查找根源带来了极大阻碍。</li><li>对nginx的error log进一步分析，发现错误大部分是在等node server response的时候超时（2000+次），还有一部分是（400+）是和node server建立connection的时候出错（应该是node server在debug的时候关掉过几次导致），socket无法建立而导致无法访问的可能性较小。目前看是建立TCP连接后node服务器响应超时的缘故。结合很高的steal cpu，可以推断node server出了问题。</li></ol><p><strong>第一阶段分析的结论和疑点：</strong></p><p>根据目前分析，大概率是node server在处理请求的时候超时导致的缘故。但是有几点比较奇怪的地方：</p><ol><li>为什么node server的cpu使用率这么高？整个请求路径是怎么样的？中间每一步的耗时情况，cpu的使用率是怎么样的？</li><li>如果只是单纯node server出了问题，为什么重启之后，问题依旧存在？而迁移到另一个服务器后问题却没了？ 怀疑还有帮凶，而且出现在系统底层。因为目前无法重现无法访问的情况，给这方面的debug带来了很大的困难。</li></ol><p><strong>一些措施：</strong></p><ol><li>研究strapi的框架，对cms的代码进行code review。在此基础上对服务进行profile搞清楚cpu使用率高的原因。</li><li>对strapi进行docker化，这样之后再出问题了，可以将部署时间极大减少。</li><li>如有可能将strapi进行拆分，将admin端和api端分开（这块不熟悉，还要看完代码才能决定）。将api端放上去kubernetes，避免单点故障。</li></ol></div><div class=pagination><a href=/xinba.github.io/posts/performance/ class="left arrow">&#8592;</a>
<a href=/xinba.github.io/posts/debug2/ class="right arrow">&#8594;</a>
<a href=# class=top>Top</a></div></main><footer><span>&copy; <time datetime="2022-03-25 03:29:54.877248298 +0000 UTC m=+0.046030045">2022</time> Alex. Made with <a href=https://gohugo.io>Hugo</a> using the <a href=https://github.com/EmielH/tale-hugo/>Tale</a> theme.</span></footer></body></html>