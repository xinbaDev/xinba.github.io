<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>关于前些天cloudflare事故的一些想法 &#183; Alex's Blog</title><link rel=stylesheet href=/xinba.github.io/css/style.css><link rel=stylesheet href=/xinba.github.io/css/fonts.css><link rel=icon href=favicon.ico><link rel=icon type=image/png sizes=32x32 href=/xinba.github.io/images/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/xinba.github.io/images/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/xinba.github.io/images/apple-touch-icon.png><link href rel=alternate type=application/rss+xml title="Alex's Blog"></head><body><nav class=nav><div class=nav-container><a href=/xinba.github.io/><h2 class=nav-title>Alex's Blog</h2></a><ul><li><a href=/xinba.github.io/posts><span>Posts</span></a></li></ul></div></nav><main><div class=post><div class=post-info><span>Written by</span>
Alex<br><span>on&nbsp;</span><time datetime="2022-06-25 10:49:00 +1100 +1100">June 25, 2022</time></div><h1 class=post-title>关于前些天cloudflare事故的一些想法</h1><div class=post-line></div><p>前几天下午cloudflare的网络出现了大事故，因为许多重要的网站都依赖cloudflare的网络，导致许多网站500无法访问，甚至连discord都一度无法正常使用。后来在twitter上看到消息，原来是cloudflare出了问题。隔天cloudflare就在我一直关注的官方博客上发布了事故报告，发布的速度和分析的详细都满足我对cloudflare这个公司的预期。这里主要是想记录一下我对这件事情的看法。</p><p>作为一个devops, cloudflare这次事故我是比较关心的。依据我对cloudflare这个公司的观察，我预计他们很快就会发布一个详细的报告，而不会像许多其他公司那样遮遮掩掩。cloudflare做为相关industry的引领者也具有这方面的自信。果然事故第二天我就在博客上看到了cloudflare详细的事故报告。根据cloudflare的描述，事故的原因是BGP的一个policy更新的时候出现了错误，因为排序的改变，导致一系列的ip无法访问。我的工作领域并没有涉及到网络底层那一块，而且也不是特别熟悉cloudflare的网络具体架构。不过从运维角度讲，其实还是有相通的地方。读完依旧让我受益不小。</p><p>首先我的第一个感想就是要管理运维好一个大的复杂的网络真是一个不简单的事情。cloudflare并不是一个一般的公司，说是行业的领头羊并不过分，具备许多优秀的程序员，网络工程师。而且任何修改都有一套发布机制，根据文章的介绍，对网络设置的修改需要开一个request ticket，经过几轮的review, 通过后先是dry run，然后在一个小的子网络发布，最后在一步步在其他网络发布。但是就算是这样，事故依然发生了。应该说这个事故并不简单，根据博文介绍，事故事发生在rollout的后部分，出现在cloudflare引入了一个新的架构上，而之前的旧架构并没有受到影响。从这里也可以看出，新的技术的引入在带来好处的同时，也带来了运维成本的提高和出错的可能性。cloudflare是一家有很强创新能力的公司，出现这种问题其实也是迟早的事情。想起来之前twitter和facebook也出现过较长时间网站无法正常运作的情况就可以看出，运维这个技术活真的不容易。</p><p>这篇文章除了事故的原因，我还有一个非常关心的地方，那就是cloudflare的应对策略，如何避免发生下次再次发生。这也是我作为一个devops在面对运维事故的时候一定会认真考虑的问题。对此，cloudflare提出了3个做法，分别从程序，架构和自动化三个方面入手。从程序来说，更新的旧的发布policy，引入对新架构的考虑。从架构上来说，杜绝出现不正确排序的情况的出现，从技术上避免同样的问题再次发生。从自动化角度上，加入相应的commit-confirm的rollback机制，减少Time to resolve的时间。可以看到，前两方面措施是避免类似事故下次在次发生的几率，后一个是减少一旦发生的影响。说到time to resolve，这次cloudflare把详细的事故timeline都附上了，甚至连中间工程师互相revert change的手忙脚乱都公开了，真是坦白的有些过头。老实说出现这种问题，尤其是出现在cloudflare这种有很强架构设计地方，往往不是好的预兆。因为一个事故，往往是有很多原因累加而导致的。就像飞机，各种设计一般来说不会发生事故。但是一旦发生事故往往就是有许多地方已经出了问题。cloudflare作为互联网重要的基础设施，其实也有类似的问题的，这是让人担心的地方。不过值得高兴的是，cloudflare对这次事故十分的坦白，从事故原因的分析，到解决方案从而避免下次发生都有详细说明，显得十分的真诚。我对cloudflare的回应是比较满意的。在一个高速发展的公司，许多新的技术架构的引进，往往会导致出错的可能行增大，这些都是可以理解的。不过我对cloudflare期望其实更高，这次事故暴露了cloudflare在运维上的某些不足。想起netfix之前为了保证服务的可靠性，专门开发了一个Chaos Monkey的开源软件，随机关闭虚拟机来测试。我认为这是每个devops应该有的mindset &mdash; 如果某个组建除了问题，可靠性是否还可以得到保证？如果实在不能如何减少影响？</p></div><div class=pagination><a href=/xinba.github.io/posts/strapi-cms/ class="left arrow">&#8592;</a>
<a href=# class=top>Top</a></div></main><footer><span>&copy; <time datetime="2022-07-15 11:06:30.644896985 +0000 UTC m=+0.056682635">2022</time> Alex. Made with <a href=https://gohugo.io>Hugo</a> using the <a href=https://github.com/EmielH/tale-hugo/>Tale</a> theme.</span></footer></body></html>